# Projeto-Final-Soulcode-tema-saude
Projeto desenvolvido em equipe como crit√©rio de aprova√ß√£o final do Curso de Engenharia de Dados pela SoulCode Academy.

Equipe do projeto:
Felipe Rodrigues Costa

Lucas

Tiago

Jefferson

# üñ•Ô∏è Tecnologias utilizadas para realiza√ß√£o do projeto:
Google Cloud Platform (CGP)
Cloud Storage (DataLake)

BigQuery (DataWarehouse)

DataStudio

Python

Pandas

PySpark

SparkSQL

Apache Beam

PostgresSQL

MongoDB

Desafio
# üóÉÔ∏è REQUISITOS OBRIGAT√ìRIOS

Obrigatoriamente os datasets devem ter formatos diferentes (CSV / Json / Parquet / Sql / NoSql) e 1 deles obrigatoriamente o que for fornecido para o projeto o mesmo ja est√° em CSV. -Converter e normalizar os dados via SPARK (csv/parquet)
Haver utiliza√ß√£o de triggers e procedures para o banco SQL

Entregar todos os scripts (DDL//DML)

Utilizar o banco NoSQL (MongoDB ou Cassandra) como um datalake

Opera√ß√µes com Pandas (limpezas , transforma√ß√µes e normaliza√ß√µes)

Opera√ß√µes usando PySpark com a descri√ß√£o de cada uma das opera√ß√µes.

Opera√ß√µes utilizando o SparkSQL com a descri√ß√£o de cada umas das opera√ß√µes.

Os datasets utilizados podem ser em lingua estrangeira , mas devem ao final terem seus dados/colunas exibidos na lingua PT-BR

Os datasets devem ser salvos e operados em armazenamento cloud obrigatoriamente dentro da plataforma GCP (n√£o pode ser usado Google drive ou armazenamento alheio ao google)

Os dados tratados devem ser armazenados tamb√©m em GCP, mas obrigatoriamente em um datalake(Gstorage ) , DW(BigQuery) ou em ambos.

Os datasets originais devem ser armazenados em MySql ou PostgresSQL

Os Dataframe(s) resultante(s) deve(m) estar em uma cole√ß√£o do mongoDb atlas (informar a key de acesso ao cluster) e preferencialmente criar o usuario (soulcode) e senha (a1b2c3) no cluster

Deve ser feito an√°lises dentro do Big Query utilizando a linguagem padr√£o SQL com a descri√ß√£o das consultas feitas.

Deve ser criado no datastudio um dashboard para exibi√ß√£o gr√°fica dos dados tratados trazendo insights importantes

E deve ser demonstrado em um workflow simples (gr√°fico) as etapas de ETL com suas respectivas ferramentas.
# üéØ REQUISITOS DESEJ√ÅVEIS

Implementar captura e ingest√£o de dados por meio de uma PIPELINE com modelo criado em apache beam usando o dataflow para o work

Utilizar o dataflow com algum modelo pr√©-definido

Criar plotagens usando pandas para alguns insights durante o processo de Transforma√ß√£o

Por meio de uma PIPELINE fazer o carregamento dos dados normalizados diretamente para um DW ou DataLake ou ambos

Montar um relat√≥rio completo com os insights que justificam todo o processo de ETL utilizado
